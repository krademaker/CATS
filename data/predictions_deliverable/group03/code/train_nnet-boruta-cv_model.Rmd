---
title: "CATS-Cross-Validation"
output: html_notebook
---
  
# The dataset consist of an aCGH (array comparative genomic hybridization) with 100 samples of three different breast cancer subtypes. The chromosomal copy number abberations have been measured by comparison to regular DNA and from this the specific type of 

```{r}
#### Load packages
library('tidyr')
library('dplyr')
library('here')

# Set root directory
ROOT_DIR = here()

#### Loading the aCGH data
# Load all the data (log2-scaled of copy number aberations between breast cancer subtypes)
train_call <- read.delim(file.path(ROOT_DIR, 'data', 'Train_call.txt'), header = TRUE, sep = '\t')
train_call$ID <- seq.int(nrow(train_call))
genomic_regions <- train_call[c('ID', 'Chromosome', 'Start', 'End', 'Nclone')]
train_call <- subset(train_call, select = -c(Chromosome,Start,End,Nclone))
train_call <- as.data.frame(t(subset(train_call, select = -c(ID))))

#### Clinical data
train_clinical <- read.delim(file.path(ROOT_DIR, 'data', 'Train_clinical.txt'), header = TRUE, sep = '\t')
row.names(train_clinical) <- train_clinical$Sample
train_clinical <- train_clinical[,-(1:1), drop=FALSE]

#### Merged clinical & array data
train_data <- merge(train_call, train_clinical, by = 'row.names')
rownames(train_data) <- train_data$Row.names
train_data$Row.names <- NULL
train_data <- train_data %>% select(Subgroup, everything())
```

# The cross-validation scheme consists of an outer loop with 10-fold stratified cross validation (70% training, 30% test), and within each outer loop is an inner loop with 10-fold stratified cross validation (70% training, 30% test). The inner loop models are used for feature selection and hyperparameter tuning of the outer loop model.

```{r}
############## Cross-Validation & Model
### Load packages
library('class')
library('gmodels')
library('caret')
library('Boruta')
library('randomForest')

### Parameters
# Outer loop CV
CV_FOLDS_OUTER <- 10
TRAINING_SIZE_OUTER <- 0.7
# Inner loop CV
CV_FOLDS_INNER <- 10
TRAINING_SIZE_INNER <- 0.7
# Neural network
NN_MAX_ITERATIONS = 100
# Variable importance
threshold.VarImp <- 0.7
# Statistical-testing
DEFAULT_SIGNIFICANCE_THRESHOLD <- 0.05
# Boruta
BORUTA_MAX_RUNS <- 1000

### Functions
# Adding clinical labels to other dataframe
attach_clinical_labels <- function(df_call, df_clinical){
  tmp_df <- merge(df_call, df_clinical, by = 'row.names')
  row.names(tmp_df) <- tmp_df$Row.names
  tmp_df$Row.names <- NULL
  return(tmp_df)
}

# Removing clinical labels from dataframe
detach_clinical_labels <- function(df){
  df$Subgroup <- NULL
  return(df)
}

# Split the input data
split <- function(train, training_size){
  trainIndex <- createDataPartition(train$Subgroup, p = training_size)
  # Save the sampleIDs with index from partition & filter out the corresponding samples for training
  resample.train <- rownames(train)[trainIndex$Resample1] 
  train_set <- filter(train, rownames(train) %in% resample.train) 
  rownames(train_set) <- resample.train 
  # Save the sampleIDs with index from partition & filter out the corresponding samples for validation
  resample.validation <- rownames(train)[-trainIndex$Resample1] 
  validation_set <- filter(train, rownames(train) %in% resample.validation) 
  rownames(validation_set) <- resample.validation 
  return(list(train_set, validation_set))
}

# Select features using filterVarImp()'s importance result
features.VarImp <- function(acgh_data, clinical_outcome){
  print(paste('Running filterVarImp feature selection, at:', Sys.time()))
  filter_variance_importances <- filterVarImp(acgh_data, clinical_outcome, nonpara = TRUE)
  filter_variance_importances$mean <- apply(filter_variance_importances, 1, mean)
  filter_variance_importances <- filter_variance_importances[order(-filter_variance_importances$mean),]
  filter_variance_importances$features <- rownames(filter_variance_importances)
  features <- as.numeric((filter(filter_variance_importances, filter_variance_importances$mean > threshold.VarImp))$features)
  return(features)
}

# Select features using Bonferroni-significant results of chi-squared statistical test
features.chi_squared <- function(acgh_data, clinical_outcome){
  print(paste('Running Pearson chi-squared test feature selection, at:', Sys.time()))
  chi_squared <- list()
  chi_squared_simulated <- list()
  for (feature in colnames(acgh_data)){
    # Extract feature data
    feature_data <- acgh_data[feature]
    colnames(feature_data) <- 'copy_number'
    # Attach clinical labels
    feature_data <- attach_clinical_labels(feature_data, clinical_outcome)
    # tally counts of all copy number-subgroup combinations
    feature_tally <- group_by(feature_data, copy_number, Subgroup) %>% tally(name = 'count')
    # Construct contigency table
    feature_contingency_table <- xtabs(count~Subgroup+copy_number, data = feature_tally)
    # Perform Pearson's chi-squared test
    chi_squared[[feature]] <- chisq.test(feature_contingency_table, simulate.p.value = TRUE)
  }
  # Set bonferroni significance threshold
  bonferroni_sign_threshold <- DEFAULT_SIGNIFICANCE_THRESHOLD / length(acgh_data)
  # Perform chi-squared statistical test with Monte Carlo simulations
  chi_squared.results <- data.frame(P_value = sapply(chi_squared, function(x) x$p.value), P_value_bonferroni = sapply(chi_squared, function(x) x$p.value / length(acgh_data)))
  chi_squared.significance <- data.frame(default = sapply(chi_squared.results$P_value, function(x) x < DEFAULT_SIGNIFICANCE_THRESHOLD), bonferroni = sapply(chi_squared.results$P_value_bonferroni, function(x) x < bonferroni_sign_threshold))
  # Selecting the features below the bonferroni significance threshold
  chi_squared.significance$features <- rownames(chi_squared.significance)
  features <- as.numeric((filter(chi_squared.significance, chi_squared.significance$bonferroni)$features))
  return(features)
}

# Select features using the Boruta algorithm
features.boruta <- function(acgh_data){
  print(paste('Running Borata feature selection, at:', Sys.time()))
  boruta <- Boruta(Subgroup~ .,
                   data = acgh_data,
                   doTrace = 1,
                   maxRuns = BORUTA_MAX_RUNS)
  boruta.tentative_fixed <- TentativeRoughFix(boruta)
  return(boruta.tentative_fixed)
}

# Predict breast cancer subgroup status with neural network
predict.nn <- function(acgh_training, acgh_test, decay = '', size = ''){
  # Handle optional hyperparameters
  if (decay == '' | size == ''){
    nnetGrid <- expand.grid(decay = c(0.1, 0.01, 0.001),  size = c(5:7))
  } else {
    nnetGrid <- expand.grid(decay = decay, size = size)
  }
  maxSize <- max(nnetGrid$size)
  acgh_training.data <- data.frame(subset(acgh_training, select = -c(Subgroup)))
  acgh_test.data <- data.frame(subset(acgh_test, select = -c(Subgroup)))
  numWts <-  1 * (maxSize * (length(acgh_training.data) + 1) + maxSize + 1)
  
  print(paste('Training neural network, at:', Sys.time()))
  nn_model <- train(acgh_training.data, acgh_training$Subgroup,
                    method = 'nnet', # train neural network using 'nnet'
                    tuneGrid = nnetGrid, # grid search hyperparameters
                    trace = FALSE,  # hide the training trace
                    MaxNWts = numWts*10, # maximum number of weights of the network
                    maxit = NN_MAX_ITERATIONS # maximum iterations
  )
  nn_model.predictions <- predict(nn_model, newdata = acgh_test.data) 
  return(list('predictions' = nn_model.predictions, 'test' = acgh_test.data, 'model' = nn_model))
}


############## Run outer- and inner-loops
final_accuracy_nn <- list()
final_performance_list = list(sensitivitiy=list(), specificity=list(), recall=list(), f1=list())

### Most outer loop - divide the training & validation set
# For a 10-fold CV
for (k in 1:CV_FOLDS_OUTER) {
  # Set new seed for every iteration
  set.seed(k)
  # Use partition function to split data set into a training & validation set
  training_test_set <- split(train_data, TRAINING_SIZE_OUTER)
  training_set <- training_test_set[[1]]
  test_set <- training_test_set[[2]]

  # Set list to store accuracies, features and other performance metrics
  accuracy_list_nn = list()
  feature_list_nn = list(feature=list())
  performance_list = list(sensitvitiy=list(), specificity=list(), recall=list(), f1=list())
  
  # List for hyperparameters
  hyperparameters_list = list(size=list(), decay=list())
  
  #### Inner loop - training
  for (i in 1:CV_FOLDS_INNER) {
    # Check progress
    print(paste('Outer-loop', k, 'with inner loop', i, 'initialzed, at:', Sys.time()))
    
    # Set new seed for every iteration
    set.seed(i*k)
    
    # New training sets for the feature selection from the training set
    training_test_set.feature <- split(training_set, TRAINING_SIZE_INNER)
    training_set.feature <- training_test_set.feature[[1]]
    test_set.feature <- training_test_set.feature[[2]]
    
    # Boruta - feature selection
    Tboruta <- features.boruta(training_set.feature)
    training_set.feature.data <- training_set.feature %>% select(-Subgroup)
    feature_selection.boruta_train <- training_set.feature.data[Tboruta$finalDecision == "Confirmed"]
    features.boruta_train <- colnames(feature_selection.boruta_train)
    feature_selection.boruta_train$Subgroup <- training_set.feature$Subgroup
    test_set.feature.data <- test_set.feature %>% select(-Subgroup)
    feature_selection.boruta_test <- test_set.feature.data[Tboruta$finalDecision == "Confirmed"]
    feature_selection.boruta_test$Subgroup <- test_set.feature$Subgroup
    
    # Accuracy values of fecature selection
    nn_model = predict.nn(feature_selection.boruta_train, feature_selection.boruta_test)
    print(paste('Finished constructing neural network, at:', Sys.time()))
    
    # Evaluate overall accuracy for feature selection
    score_nn = confusionMatrix(test_set.feature$Subgroup, nn_model$predictions)
    
    # Evaluate performance per class for feature selection
    performance.df <- as.data.frame(score_nn[["byClass"]])
    performance_list$sensitivity[[i]] <- performance.df$Sensitivity
    performance_list$specificity[[i]] <- performance.df$Specificity
    performance_list$recall[[i]] <- performance.df$Recall
    performance_list$f1[[i]] <- performance.df$F1
    # Append the accuracy and corresponding features to lists
    accuracy_list_nn[i] <- score_nn[["overall"]][["Accuracy"]]
    feature_list_nn$feature[[i]] <- features.boruta_train
    
    # Store hyperparameters in list
    hyperparameters_list$size[[i]] <- nn_model$model$bestTune$size
    hyperparameters_list$decay[[i]] <- nn_model$model$bestTune$decay
  }
  avgs_vector <- unlist(accuracy_list_nn)
  top_index <- which.max(avgs_vector)
  top_features <- feature_list_nn$feature[[top_index]]
  
  training.features <- training_set[,top_features]
  test.features <- test_set[,top_features]
  
  training_set <- cbind(training_set$Subgroup, training.features)
  test_set <- cbind(test_set$Subgroup, test.features)
  colnames(training_set)[1] <- 'Subgroup'
  colnames(test_set)[1] <- 'Subgroup'
  
  # Neural network - outer loop
  best_size <- unlist(hyperparameters_list$size)[top_index]
  best_decay <- unlist(hyperparameters_list$decay)[top_index]
  nn_model = predict.nn(training_set, test_set, best_decay, best_size)
  score_nn.inner = confusionMatrix(test_set$Subgroup, nn_model$predictions)
  
  # Accuracy
  acc_nn = score_nn[["overall"]][["Accuracy"]]
  final_accuracy_nn[k] <- acc_nn
  
  # Performance metrics - Precision, Recall, Sensitvity, Specificity per class
  final_performance.df <- as.data.frame(score_nn.inner[["byClass"]])
  final_performance_list$sensitivity[[k]] <- performance.df$Sensitivity
  final_performance_list$specificity[[k]] <- performance.df$Specificity
  final_performance_list$recall[[k]] <- performance.df$Recall
  final_performance_list$f1[[k]] <- performance.df$F1
  
  # Save the models in a RDS file
  model_rds_path <- file.path(root_dir, paste('nn_model_', k, '.rds', sep = ''))
  saveRDS(nn_model$model, model_rds_path)
  print(paste('Saved neural network model to RDS file:', model_rds_path, ', at:', Sys.time()))
}

best_accuracy_index <- which.max(unlist(final_accuracy_nn))
print(paste('The best performing model (by accuracy) is model #', best_accuracy_index, 'at an accuracy of:', final_accuracy_nn[[best_accuracy_index]]))
print(paste('The overall accuracy of all models is:', mean(unlist(final_accuracy_nn), na.rm = TRUE)))
```
