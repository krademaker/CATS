---
---
  title: "CATS-Cross-Validation"
output: html_notebook
---
  
# The dataset consist of an aCGH (array comparative genomic hybridization) with 100 samples of three different breast cancer subtypes. The chromosomal copy number abberations have been measured by comparison to regular DNA and from this the specific type of 

```{r}
#### Install packages
library(tidyr)
library(dplyr)
library(here)

root_dir = here()
#### Loading the aCGH data
# Load all the data (log2-scaled of copy number aberations between breast cancer subtypes)
train_call <- read.delim(file.path(root_dir, 'data', 'train_call.tsv'), header = TRUE, sep='\t')

train_call$ID <- seq.int(nrow(train_call))
genomic_regions <- train_call[c('ID','Chromosome','Start','End','Nclone')]

train_call <- subset(train_call, select = -c(Chromosome,Start,End,Nclone))
train_call <- as.data.frame(t(subset(train_call, select = -c(ID))))

#### Clinical data
train_clinical <- read.delim(file.path(root_dir, 'data', "train_clinical.tsv"), header=TRUE, sep="\t")
row.names(train_clinical) <- train_clinical$Sample
train_clinical <- train_clinical[,-(1:1), drop=FALSE]

#### Merged clinical & array data
train_data <- merge(train_call, train_clinical, by='row.names')
rownames(train_data) <- train_data$Row.names
train_data$Row.names <- NULL
train_data <- train_data %>% select(Subgroup, everything())

```

# For this part I will provide a cross-validation scheme. We will first split the data into a test and a training set. The training set will be used to do feature selection and create the model with various classification algorithms.

#For the scheme, we will split the data set into a test (for validation of final predictor) and training set (from which we select features and build the classification model). This outer loop will be 10-fold cross-validated


```{r}
############## Cross-Validation & Model
### Load packages
library('class')
library('gmodels')
library('caret')
library('Boruta')
library('randomForest')
### Parameters
#Outer loop CV
cv_fold_outer <- 10
training_size_outer <- 0.7
validation_size_outer <- 1 - training_size_outer
#Inner loop CV
cv_fold_inner <- 10
feature_size <- 0.7
#KNN
k_number = 10
#Variable importance
threshold.VarImp <- 0.7
#Statistical-testing
default_sign_threshold <- 0.05
### Functions

#Adding clinical labels to other dataframe
attach_clinical_labels <- function(df_call, df_clinical){
  tmp_df <- merge(df_call, df_clinical, by = 'row.names')
  row.names(tmp_df) <- tmp_df$Row.names
  tmp_df$Row.names <- NULL
  return(tmp_df)
}

#Removing clinical labels from dataframe
detach_clinical_labels <- function(df){
  df$Subgroup <- NULL
  return(df)
}
# Split the input data
split <- function(train, training_size){
  trainIndex <- createDataPartition(train$Subgroup, p = training_size)
  #Save the sampleIDs with index from partition & filter out the corresponding samples for training
  resample.train <- rownames(train)[trainIndex$Resample1] 
  train_set <- filter(train, rownames(train) %in% resample.train) 
  rownames(train_set) <- resample.train 
  #Save the sampleIDs with index from partition & filter out the corresponding samples for validation
  resample.validation <- rownames(train)[-trainIndex$Resample1] 
  validation_set <- filter(train, rownames(train) %in% resample.validation) 
  rownames(validation_set) <- resample.validation 
  return(list(train_set, validation_set))
}

features.VarImp <- function(data, clinical_outcome){
  filter_variance_importances <- filterVarImp(data, clinical_outcome, nonpara = TRUE)
  filter_variance_importances$mean <- apply(filter_variance_importances, 1, mean)
  filter_variance_importances <- filter_variance_importances[order(-filter_variance_importances$mean),]
  filter_variance_importances$features <- rownames(filter_variance_importances)
  features <- as.numeric((filter(filter_variance_importances, filter_variance_importances$mean > threshold.VarImp))$features)
  return(features)
}
features.chi_squared <- function(data, clinical_outcome){
  chi_squared <- list()
  chi_squared_simulated <- list()
  for (feature in colnames(data)){
    # Extract feature data
    feature_data <- data[feature]
    colnames(feature_data) <- 'copy_number'
    # Attach clinical labels
    feature_data <- attach_clinical_labels(feature_data, clinical_outcome)
    # tally counts of all copy number-subgroup combinations
    feature_tally <- group_by(feature_data, copy_number, Subgroup) %>% tally(name = 'count')
    # Construct contigency table
    feature_contingency_table <- xtabs(count~Subgroup+copy_number, data = feature_tally)
    # Perform Pearson's chi-squared test
    chi_squared[[feature]] <- chisq.test(feature_contingency_table, simulate.p.value = TRUE)
  }
  # Set bonferroni significance threshold
  bonferroni_sign_threshold <- default_sign_threshold / length(data)
  # Perform chi-squared statistical test with Monte Carlo simulations
  chi_squared.results <- data.frame(P_value = sapply(chi_squared, function(x) x$p.value), P_value_bonferroni = sapply(chi_squared, function(x) x$p.value / length(data)))
  chi_squared.significance <- data.frame(default = sapply(chi_squared.results$P_value, function(x) x < default_sign_threshold), bonferroni = sapply(chi_squared.results$P_value_bonferroni, function(x) x < bonferroni_sign_threshold))
  # Selecting the features below the bonferroni significance threshold
  chi_squared.significance$features <- rownames(chi_squared.significance)
  features <- as.numeric((filter(chi_squared.significance, chi_squared.significance$bonferroni)$features))
  return (features)
}

features.boruta <- function(acgh_data){
  print('Running Boruta...')
  boruta <- Boruta(Subgroup~ ., data = acgh_data, doTrace = 1, maxRuns = 1000)
  boruta.tentative_fixed <- TentativeRoughFix(boruta)
  return(boruta.tentative_fixed)
}

####Example with KNN
prediction <- function(training, test, features) {
  #Select columns with selected features only
  
  selected.train <- select(training, features)
  selected.test <- select(test, features)
  cl <- training$Subgroup
  
  #Make predictions
  prediction <- knn(selected.train[,-c(1)], selected.test[,-c(1)], cl, k=k_number, prob=TRUE)
  return(list('predictions'=prediction, 'test'=selected.test))
}

pred_nn <- function(training_set, test_set, decay = "", size = ""){
  #CV
  ctrl <- trainControl(method = "cv",  # cross-validation
                       number = 10,  # 10 folds
                       classProbs = FALSE , # report class probability
  )
  
  if (decay == "" | size == ""){
    nnetGrid <- expand.grid(decay = c(0.1, 0.01, 0.001),  size = c(5:7))
  } else {
    nnetGrid <- expand.grid(decay, size)
  }
  maxSize <- max(nnetGrid$size)
  training_set.data <- data.frame(subset(training_set, select =   -c(Subgroup)))
  test_set.data <- data.frame(subset(test_set, select = -c(Subgroup)))
  
  numWts <- 1*(maxSize * (length(training_set.data) + 1) + maxSize + 1)
  print(numWts)
  print("NN working")
  model.nn <- train(training_set.data, training_set$Subgroup,
                    method = "nnet", # train neural network using `nnet`
                    tuneGrid = nnetGrid, # tuning grid
                    #preProc = c("scale"), # standardize data
                    trace = FALSE,  # hide the training trace
                    #trControl = ctrl,
                    MaxNWts = numWts*10,
                    maxit = 100 # maximum iteration
  )
  plot(model.nn)
  print(model.nn)
  predictions.nn <- predict(model.nn, newdata = test_set.data) 
    
  return(list('predictions'=predictions.nn, 'test'=test_set, 'model'=n, 'size' = model.nn$size, 'decay' = model.nn$decay))
}
#####Start program


final_accuracy_nn <- list()
final_performance_list = list(sensitivitiy=list(), specificity=list(), recall=list(), f1=list())

### Most outer loop - divide the training & validation set
#For a 10-fold CV
for (k in 1:cv_fold_outer) {
  #Set new seed for every iteration
  set.seed(k)
  #Use partition function to split data set into a training & validation set
  training_test_set <- split(train_data, training_size_outer)
  training_set <- training_test_set[[1]]
  test_set <- training_test_set[[2]]

  #Set list to store accuracies, features and other performance metrics
  accuracy_list_nn = list()
  feature_list_nn = list(feature=list())
  performance_list = list(sensitvitiy=list(), specificity=list(), recall=list(), f1=list())
  
  #list for hyperparameters
  hyperparameters_list = list(size=list(), decay=list())
  
  ####Inner loop - training
  for (i in 1:cv_fold_inner) {
    #Check progress
    print(k)
    print(i)
    
    #Set new seed for every iteration
    set.seed(69+i)
    
    #New training sets for the feature selection from the training set
    training_test_set.feature <- split(training_set, feature_size)
    training_set.feature <- training_test_set.feature[[1]]
    test_set.feature <- training_test_set.feature[[2]]
    
    #Boruta - feature selection
    Tboruta <- features.boruta(training_set.feature)
    training_set.feature.data <- training_set.feature %>% select(-Subgroup)
    feature_selection.boruta_train <- training_set.feature.data[Tboruta$finalDecision == "Confirmed"]
    features.boruta_train <- colnames(feature_selection.boruta_train)
    feature_selection.boruta_train$Subgroup <- training_set.feature$Subgroup
    test_set.feature.data <- test_set.feature %>% select(-Subgroup)
    feature_selection.boruta_test <- test_set.feature.data[Tboruta$finalDecision == "Confirmed"]
    feature_selection.boruta_test$Subgroup <- test_set.feature$Subgroup
    
    #Accuracy values of fecature selection
    nn_model = pred_nn(feature_selection.boruta_train, feature_selection.boruta_test)
    print("NN finished")
    
    #Evaluate overall accuracy for feature selection
    score_nn = confusionMatrix(test_set.feature$Subgroup, nn_model$predictions)
    
    #Evaluate performance per class for feature selection
    performance.df <- as.data.frame(score_nn.inner[["byClass"]])
    performance_list$sensitivity[[i]] <- performance.df$Sensitivity
    performance_list$specificity[[i]] <- performance.df$Specificity
    performance_list$recall[[i]] <- performance.df$Recall
    performance_list$f1[[i]] <- performance.df$F1
    #Append the accuracy and corresponding features to lists
    accuracy_list_nn[i] <- score_nn[["overall"]][["Accuracy"]]
    feature_list_nn$feature[[i]] <- features.boruta_train
    
    #Store hyperparameters in list
    hyperparameters_list$size[[i]] <- nn_model$size
    hyperparameters_list$decay[[i]] <- nn_model$decay
  }
  avgs_vector <- unlist(accuracy_list_nn)
  top_index <- which.max(avgs_vector)
  top_features <- feature_list_nn$feature[[top_index]]
  
  training.features <- training_set[,top_features]
  test.features <- test_set[,top_features]
  
  training_set <- cbind(training_set$Subgroup, training.features)
  test_set <- cbind(test_set$Subgroup, test.features)
  colnames(training_set)[1] <- 'Subgroup'
  colnames(test_set)[1] <- 'Subgroup'
  
  #Neural network - outer loop
  nn_model = pred_nn(training_set, test_set, hyperparameters_list$decay[[top_index]], hyperparameters_list$size[[top_index]])

  score_nn.inner = confusionMatrix(test_set$Subgroup, nn_model$predictions)
  #Accuracy
  acc_nn = score_nn[["overall"]][["Accuracy"]]
  final_accuracy_nn[k] <- acc_nn
  
  #Performance metrics - Precision, Recall, Sensitvity, Specificity per class
  final_performance.df <- as.data.frame(score_nn.inner[["byClass"]])
  final_performance_list$sensitivity[[i]] <- performance.df$Sensitivity
  final_performance_list$specificity[[i]] <- performance.df$Specificity
  final_performance_list$recall[[i]] <- performance.df$Recall
  final_performance_list$f1[[i]] <- performance.df$F1
}

mean(unlist(final_accuracy_nn), na.rm = TRUE)

```
